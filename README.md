ðŸŒ¿ GPT-2 Fine-Tuning on Nature Text

This project is part of my PRODIGY AI Internship â€“ Task 1, where I explored Generative AI concepts by fine-tuning a GPT-2 model using nature-related text data.

ðŸ“Œ What I Learned

ðŸ¤– Generative AI (GenAI) basics

ðŸŒ€ GANs (Generative Adversarial Networks) vs VAEs (Variational Autoencoders)

ðŸ”‘ Transformers and how they power modern LLMs

ðŸ§  Fine-tuning pre-trained models like GPT-2 for specific tasks

ðŸ“‚ Handling datasets & text preprocessing in Google Colab

ðŸ“’ Project Files

Task1_GPT2_FineTuning.py â†’ Python script with fine-tuning code

data.txt â†’ Nature text dataset used for training

README.md â†’ Project documentation (this file âœ¨)

ðŸš€ How to Run

1.Clone the repo:

git clone https://github.com/Malini0605/PRODIGY_AI_01.git
cd PRODIGY_AI_01


2.Install dependencies:

pip install transformers torch


3.Run training:

python Task1_GPT2_FineTuning.py

ðŸ“· Demo

task video here ["C:\Users\USER\Videos\Captures\video.mp4"]

ðŸŒŸ Acknowledgements

Thanks to Prodigy Infotech for this opportunity to explore Generative AI.
